{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%matplotlib InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, accuracy_score\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "sns.set(context='notebook', style='whitegrid', font_scale=1.2)\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_signature', 'chorus_hit', 'sections', 'target', 'popularity',\n",
       "       'sm_target', 'tiktok', 'spotify', 'duration_ms', 'danceability',\n",
       "       'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
       "       'instrumentalness', 'liveness', 'valence', 'tempo', 'era',\n",
       "       'main_parent_genre'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Spotify Data/data-clean.csv')\n",
    "\n",
    "#drop track, artist, track_id \n",
    "df = df.drop(['track', 'artist', 'track_id'], axis=1)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = map(str.lower, df.columns)\n",
    "df['track_seconds'] = df['duration_ms'] / 1000\n",
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing confusion matrices (see: https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823)\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (7,5), fontsize=10):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=(7,5))\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"mako\") #mako\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "    plt.ylabel('True label',fontsize=14)\n",
    "    plt.xlabel('Predicted label',fontsize=14)\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"xg_confmatrix.png\")\n",
    "    plt.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to plot the learning curve, roc curve, and confusing matrix for the given model\n",
    "def plot_model(model, X, y , X2, y2,threshold=0.5):\n",
    "    model.fit(X, y)\n",
    "    y_predict = model.predict(X)\n",
    "    #y_predictprob = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    y_predictprob = (model.predict_proba(X)[:, 1] >=threshold).astype('int')\n",
    "\n",
    "    LR_acc = np.round(np.mean(cross_val_score(model, X, y, scoring = 'accuracy', cv = 5)), 5)\n",
    "    LR_f1 = np.round(np.mean(cross_val_score(model, X, y, scoring = 'f1', cv = 5)), 5)\n",
    "    LR_prec = np.round(np.mean(cross_val_score(model, X, y, scoring = 'precision', cv = 5)), 5)\n",
    "    LR_recall = np.round(np.mean(cross_val_score(model, X, y, scoring = 'recall', cv = 5)), 5)\n",
    "    print(f'The base LR accuracy is: {LR_acc}')\n",
    "    print(f'The base LR f1 is: {LR_f1}')\n",
    "    print(f'The base LR precision is: {LR_prec}')\n",
    "    print(f'The base LR recall is: {LR_recall}')\n",
    "\n",
    "    #print(\"train : all metrics:\", metrics.classification_report(y,y_predictprob))\n",
    "\n",
    "    print(\"Precision: {:6.4f},   Recall: {:6.4f}, Accuracy: {:6.4f}\".format(precision_score(y, y_predictprob),\n",
    "                                                        recall_score(y, y_predictprob) ,\n",
    "                                                        accuracy_score(y,y_predictprob)))\n",
    "    \n",
    "    \n",
    "    m, train_err, test_err = learning_curve(model, X, y, cv = 5, scoring = 'f1', random_state = 42)\n",
    "    m_trainerr = np.mean(train_err, axis = 1)\n",
    "    m_testerr = np.mean(test_err, axis = 1)\n",
    "\n",
    "    print(\"train error\", m_trainerr.mean(), \"test error\", m_testerr.mean())\n",
    "    print(\"y pred proba \",y_predictprob)\n",
    "    \n",
    "    \n",
    "    # try\n",
    "    \n",
    "    ns2_probs = [0 for _ in range(len(y2))]\n",
    "    y2_predict = model.predict(X2)\n",
    "    y2_predictprobs = model.predict_proba(X2)[:, 1]\n",
    "    y2_predictprob = (model.predict_proba(X2)[:, 1]>=threshold).astype('int')\n",
    "    \n",
    "    #print(\"test : all metrics: \", metrics.classification_report(y2,y2_predictprob))    \n",
    "    print(\"Precision: {:6.4f},   Recall: {:6.4f}, Accuracy: {:6.4f}, f1: {:6.4f}\".format(precision_score(y2, y2_predictprob),\n",
    "                                                        recall_score(y2, y2_predictprob) ,\n",
    "                                                        accuracy_score(y2,y2_predictprob),\n",
    "                                                        f1_score(y2,y2_predictprob)))\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    connect predictions with outputs for sample probablity\n",
    "    '''\n",
    "    global y_pred_prob_df \n",
    "    global y_pred_df\n",
    "    y3_predictprob = (model.predict_proba(X2)[:, 1]) \n",
    "    \n",
    "#     print(\"X inputs: \", X2[:10], type(X2))\n",
    "#     print(\"y predicted prob: \", y3_predictprob[:10], type(y3_predictprob))\n",
    "#     print(\"y predicted outcomes: \", y2_predictprob[:10], type(y2_predictprob))\n",
    "#     print(\"y actuals: \",y2[:10] , type(y2))\n",
    "    \n",
    "    y_pred_prob_df = pd.DataFrame(y3_predictprob,columns=['y_pred_prob'])\n",
    "    y_pred_df = pd.DataFrame(y2_predictprob,columns=['y_pred'])    \n",
    "    \n",
    "    '''\n",
    "    change array and series to df\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thr = roc_curve(y2, y2_predictprobs)\n",
    "    \n",
    "    ns2_auc = roc_auc_score(y2, ns2_probs)\n",
    "    ns2_fpr, ns2_tpr, _ = roc_curve(y2, ns2_probs)\n",
    "\n",
    "#     optimal_idx = np.argmax(tpr - fpr)\n",
    "#     optimal_threshold = thresholds[optimal_idx]\n",
    "#     print(\"optimal idx\", optimal_idx, \"optimal threshold\", optimal_threshold)\n",
    "    \n",
    "    plt.rcParams.update({'figure.figsize': (7, 5)})\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.plot(m, m_trainerr, 'k', m, m_testerr, 'r')\n",
    "#     plt.xlabel('Number of Samples', fontsize = 10)\n",
    "#     plt.ylabel('F1', fontsize = 10)\n",
    "#     plt.legend(['Training Error', 'Test Error'])\n",
    "    #plt.subplot(1,2,2)\n",
    "    \n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('1 - Specificity (FPR)')\n",
    "    plt.ylabel('Sensitivity (TPR)');\n",
    "    plt.title(f\"Area Under the ROC Curve: {np.round(roc_auc_score(y2, y2_predictprobs), 4)}\");\n",
    "\n",
    "    print(f'AUC: {np.round(roc_auc_score(y2, y2_predictprobs), 4)}')\n",
    "    \n",
    "    pyplot.plot(ns2_fpr, ns2_tpr, linestyle='--', label='No Skill')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"xg_aucroc.pdf\")\n",
    "    \n",
    "    \n",
    "    conf_mat = confusion_matrix(y2, y2_predictprob)\n",
    "    cm = print_confusion_matrix(conf_mat, ['not a hit', 'is a hit'])\n",
    "    \n",
    "    return y_pred_prob_df \n",
    "    return y_pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_signature', 'chorus_hit', 'sections', 'target', 'popularity',\n",
       "       'sm_target', 'tiktok', 'spotify', 'duration_ms', 'danceability',\n",
       "       'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
       "       'instrumentalness', 'liveness', 'valence', 'tempo', 'era',\n",
       "       'main_parent_genre', 'track_seconds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establishing X and y\n",
    "y = df['target']\n",
    "X = df.drop(columns = ['target','duration_ms', 'mode', 'key', 'era'])\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "X2_train, X2_test,  y2_train,y2_test = train_test_split(X_train,y_train, test_size = 0.25, random_state = 42)\n",
    "\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pylab\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base LR accuracy is: 0.96771\n",
      "The base LR f1 is: 0.9863\n",
      "The base LR precision is: 0.97837\n",
      "The base LR recall is: 0.99444\n",
      "Precision: 1.0000,   Recall: 1.0000, Accuracy: 1.0000\n",
      "train error nan test error nan\n",
      "y pred proba  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "Precision: 1.0000,   Recall: 0.9841, Accuracy: 0.9841, f1: 0.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m xgb_base \u001b[39m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m----> 2\u001b[0m plot_model(xgb_base, X2_train, y2_train, X2_test, y2_test)\n",
      "Cell \u001b[0;32mIn[7], line 68\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, X, y, X2, y2, threshold)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m    change array and series to df\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     fpr, tpr, thr \u001b[39m=\u001b[39m roc_curve(y2, y2_predictprobs)\n\u001b[0;32m---> 68\u001b[0m     ns2_auc \u001b[39m=\u001b[39m roc_auc_score(y2, ns2_probs)\n\u001b[1;32m     69\u001b[0m     ns2_fpr, ns2_tpr, _ \u001b[39m=\u001b[39m roc_curve(y2, ns2_probs)\n\u001b[1;32m     71\u001b[0m \u001b[39m#     optimal_idx = np.argmax(tpr - fpr)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m#     optimal_threshold = thresholds[optimal_idx]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m#     print(\"optimal idx\", optimal_idx, \"optimal threshold\", optimal_threshold)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[1;32m    571\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[1;32m    574\u001b[0m         y_true,\n\u001b[1;32m    575\u001b[0m         y_score,\n\u001b[1;32m    576\u001b[0m         average,\n\u001b[1;32m    577\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[1;32m    582\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[1;32m    586\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[1;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:339\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39msample_weight)\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "xgb_base = GradientBoostingClassifier()\n",
    "plot_model(xgb_base, X2_train, y2_train, X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV For Gradient Boosting\n",
    "grad = GradientBoostingClassifier()\n",
    "grad_param = {'n_estimators':[100],\n",
    "              'max_depth':[3,5],\n",
    "              #'max_features': [6,10],\n",
    "              #'min_samples_split': [None], \n",
    "              #'min_samples_leaf':[3,10],\n",
    "              'learning_rate': [0.1] #0.0001, 0.001, 0.01, \n",
    "             }\n",
    "\n",
    "grad_grid = GridSearchCV(grad, param_grid=grad_param, cv=3, scoring='f1', verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = grad_grid.fit(X2_train,y2_train)\n",
    "\n",
    "grad_ypred = grad_grid.predict(X2_test)\n",
    "\n",
    "grad_yproba = grad_grid.predict_proba(X2_test)[:,1]\n",
    "\n",
    "fpr_grad, tpr_grad, _ = roc_curve(y2_test, grad_yproba)\n",
    "\n",
    "print('best score: ',xgb1.best_score_)\n",
    "print('best params: ',xgb1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2=GradientBoostingClassifier(                   \n",
    "                   max_depth=3, \n",
    "                   max_features=4,\n",
    "                   max_leaf_nodes=None,                   \n",
    "                   #min_samples_leaf=10,\n",
    "                   n_estimators=100,\n",
    "                   learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2b = xgb2.fit(X2_train,y2_train)\n",
    "\n",
    "grad_ypred = xgb2.predict(X2_test)\n",
    "\n",
    "grad_yproba = xgb2.predict_proba(X2_test)[:,1]\n",
    "\n",
    "fpr_grad, tpr_grad, _ = roc_curve(y2_test, grad_yproba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(xgb2, X2_train, y2_train, X2_test, y2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_importance_top10(model, X_train, top_n=10):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pandas import DataFrame\n",
    "    imp = DataFrame({\"imp\": model.feature_importances_, \"names\": X_train.columns}).sort_values(\"imp\", ascending=True)\n",
    "    imp_top_n = imp[-top_n:]\n",
    "    fig, ax = plt.subplots(figsize=(imp_top_n.shape[0]/6, imp_top_n.shape[0]/5), dpi=300) \n",
    "    ax.barh(imp_top_n[\"names\"], imp_top_n[\"imp\"], color=\"green\") \n",
    "   # ax.set_xlabel('\\nVariable Importance')\n",
    "   # ax.set_ylabel('Features\\n') \n",
    "    ax.set_title('Variable Importance Plot Top 10\\n') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance_top10(xgb2, X2_train, top_n=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFITTING BASED ON DROPPED COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = X2_train.drop(['tempo','speechiness','energy', 'sections'], axis = 1) #mode, key to be dropped here\n",
    "X3_test = X2_test.drop(['tempo','speechiness','energy', 'sections'], axis = 1)\n",
    "# rlr_secondf1 = plot_model(lr, lr_Xtrain, y2_train, X2_test, y2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "plot_model(xgb2, X3_train, y2_train, X3_test, y2_test)\n",
    "pickle.dump(xgb2, open('xgb_model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(xgb2, X3_train, y2_train, X3_test, y2_test, 0.66) # changing threshold to 0.66 for more precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF for predicted prob/outcomes and merge back to orig df\n",
    "y_pred_prob_df['row_num']= np.arange(len(y_pred_prob_df))\n",
    "\n",
    "y_pred_df['row_num']= np.arange(len(y_pred_df))\n",
    "X_test['row_num']= np.arange(len(X_test))\n",
    "y_test_df = y_test.to_frame()\n",
    "y_test_df['m_index'] = y_test_df.index\n",
    "#y_test_df2=y_test_df.rename_axis('m_index')\n",
    "y_test_df['row_num']= np.arange(len(y_test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "data_frames=[X_test,y_test_df,y_pred_df,y_pred_prob_df]\n",
    "m_results = reduce(lambda left,right: pd.merge(left,right,on=['row_num'], how='outer'),data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_index'] = df.index\n",
    "final_df = pd.merge(df,m_results,left_on='m_index',right_on='m_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(r'sb_final_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_predict = xgb2.predict(X3_test) #not sure if X3_test is correct\n",
    "hit_score = xgb2.predict_proba(X3_test) #not sure if X3_test is correct\n",
    "print(f'prediction: {hit_predict[0]} \\nprobability of yay: {hit_score[0][1]}\\nprobability of nay: {hit_score[0][0]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
