{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "#%matplotlib InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, accuracy_score\n",
    "\n",
    "# %config InlineBackend.figure_formats = ['svg']\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "sns.set(context='notebook', style='whitegrid', font_scale=1.2)\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_signature</th>\n",
       "      <th>chorus_hit</th>\n",
       "      <th>sections</th>\n",
       "      <th>target</th>\n",
       "      <th>popularity</th>\n",
       "      <th>sm_target</th>\n",
       "      <th>tiktok</th>\n",
       "      <th>spotify</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>era</th>\n",
       "      <th>main_parent_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.94975</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173533.0</td>\n",
       "      <td>0.417</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.727</td>\n",
       "      <td>major</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.845</td>\n",
       "      <td>185.655</td>\n",
       "      <td>60s</td>\n",
       "      <td>Blues and Jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>48.82510</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>213613.0</td>\n",
       "      <td>0.498</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.475</td>\n",
       "      <td>major</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.797</td>\n",
       "      <td>101.801</td>\n",
       "      <td>60s</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_signature  chorus_hit  sections  target  popularity  sm_target  \\\n",
       "0             3.0    32.94975       9.0     1.0         NaN        0.0   \n",
       "1             4.0    48.82510      10.0     0.0         NaN        0.0   \n",
       "\n",
       "   tiktok  spotify  duration_ms  danceability  ...  loudness   mode  \\\n",
       "0       0        1     173533.0         0.417  ...    -7.727  major   \n",
       "1       0        1     213613.0         0.498  ...   -12.475  major   \n",
       "\n",
       "   speechiness acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0403        0.490             0.000    0.0779    0.845  185.655   \n",
       "1       0.0337        0.018             0.107    0.1760    0.797  101.801   \n",
       "\n",
       "   era  main_parent_genre  \n",
       "0  60s     Blues and Jazz  \n",
       "1  60s               Rock  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Spotify Data/data-clean.csv')\n",
    "\n",
    "#drop track, artist, track_id \n",
    "df = df.drop(['track', 'artist', 'track_id'], axis=1)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40560, 23)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = map(str.lower, df.columns)\n",
    "df['track_seconds'] = df['duration_ms'] / 1000\n",
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing confusion matrices (see: https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823)\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (7,5), fontsize=10):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=(7,5))\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"mako\") #mako\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "    plt.ylabel('True label',fontsize=14)\n",
    "    plt.xlabel('Predicted label',fontsize=14)\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"xg_confmatrix.png\")\n",
    "    plt.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40560, 14)\n"
     ]
    }
   ],
   "source": [
    "# Establishing X and y\n",
    "# df = df.drop(columns = ['duration_ms', 'mode', 'key', 'era', 'main_parent_genre', 'sm_target', 'tiktok','spotify'  ])\n",
    "\n",
    "\n",
    "# y = df['target']\n",
    "# X = df.drop(columns = ['target'])\n",
    "\n",
    "# # Train test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "# X2_train, X2_test,  y2_train,y2_test = train_test_split(X_train,y_train, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# #historgram of values of y and show it\n",
    "# X.shape\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = df.drop(columns=['duration_ms', 'mode', 'key', 'era', 'main_parent_genre', 'sm_target', 'tiktok', 'spotify'])\n",
    "\n",
    "y = df['target']\n",
    "X = df.drop(columns=['target'])\n",
    "\n",
    "# Handling missing values\n",
    "imputer = SimpleImputer(strategy='mean')  # Replace missing values with the mean\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shape of X to verify the fix\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import pylab\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function used to plot the learning curve, roc curve, and confusing matrix for the given model\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_model(model, X, y , X2, y2,threshold=0.5):\n",
    "#     model.fit(X, y)\n",
    "#     y_predict = model.predict(X)\n",
    "#     #y_predictprob = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "#     y_predictprob = (model.predict_proba(X)[:, 1] >=threshold).astype('int')\n",
    "\n",
    "#     LR_acc = np.round(np.mean(cross_val_score(model, X, y, scoring = 'accuracy', cv = 5)), 5)\n",
    "#     LR_f1 = np.round(np.mean(cross_val_score(model, X, y, scoring = 'f1', cv = 5)), 5)\n",
    "#     LR_prec = np.round(np.mean(cross_val_score(model, X, y, scoring = 'precision', cv = 5)), 5)\n",
    "#     LR_recall = np.round(np.mean(cross_val_score(model, X, y, scoring = 'recall', cv = 5)), 5)\n",
    "#     print(f'The base LR accuracy is: {LR_acc}')\n",
    "#     print(f'The base LR f1 is: {LR_f1}')\n",
    "#     print(f'The base LR precision is: {LR_prec}')\n",
    "#     print(f'The base LR recall is: {LR_recall}')\n",
    "\n",
    "#     #print(\"train : all metrics:\", metrics.classification_report(y,y_predictprob))\n",
    "\n",
    "#     print(\"Precision: {:6.4f},   Recall: {:6.4f}, Accuracy: {:6.4f}\".format(precision_score(y, y_predictprob),\n",
    "#                                                         recall_score(y, y_predictprob) ,\n",
    "#                                                         accuracy_score(y,y_predictprob)))\n",
    "    \n",
    "    \n",
    "#     m, train_err, test_err = learning_curve(model, X, y, cv = 5, scoring = 'f1', random_state = 42)\n",
    "#     m_trainerr = np.mean(train_err, axis = 1)\n",
    "#     m_testerr = np.mean(test_err, axis = 1)\n",
    "\n",
    "#     print(\"train error\", m_trainerr.mean(), \"test error\", m_testerr.mean())\n",
    "#     print(\"y pred proba \",y_predictprob)\n",
    "    \n",
    "    \n",
    "#     # try\n",
    "    \n",
    "#     ns2_probs = [0 for _ in range(len(y2))]\n",
    "#     y2_predict = model.predict(X2)\n",
    "#     y2_predictprobs = model.predict_proba(X2)[:, 1]\n",
    "#     y2_predictprob = (model.predict_proba(X2)[:, 1]>=threshold).astype('int')\n",
    "    \n",
    "#     #print(\"test : all metrics: \", metrics.classification_report(y2,y2_predictprob))    \n",
    "#     print(\"Precision: {:6.4f},   Recall: {:6.4f}, Accuracy: {:6.4f}, f1: {:6.4f}\".format(precision_score(y2, y2_predictprob),\n",
    "#                                                         recall_score(y2, y2_predictprob) ,\n",
    "#                                                         accuracy_score(y2,y2_predictprob),\n",
    "#                                                         f1_score(y2,y2_predictprob)))\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     connect predictions with outputs for sample probablity\n",
    "#     '''\n",
    "#     global y_pred_prob_df \n",
    "#     global y_pred_df\n",
    "#     y3_predictprob = (model.predict_proba(X2)[:, 1]) \n",
    "    \n",
    "# #     print(\"X inputs: \", X2[:10], type(X2))\n",
    "# #     print(\"y predicted prob: \", y3_predictprob[:10], type(y3_predictprob))\n",
    "# #     print(\"y predicted outcomes: \", y2_predictprob[:10], type(y2_predictprob))\n",
    "# #     print(\"y actuals: \",y2[:10] , type(y2))\n",
    "    \n",
    "#     y_pred_prob_df = pd.DataFrame(y3_predictprob,columns=['y_pred_prob'])\n",
    "#     y_pred_df = pd.DataFrame(y2_predictprob,columns=['y_pred'])    \n",
    "    \n",
    "#     '''\n",
    "#     change array and series to df\n",
    "#     '''\n",
    "    \n",
    "#     fpr, tpr, thr = roc_curve(y2, y2_predictprobs)\n",
    "    \n",
    "#     ns2_auc = roc_auc_score(y2, ns2_probs)\n",
    "#     ns2_fpr, ns2_tpr, _ = roc_curve(y2, ns2_probs)\n",
    "\n",
    "# #     optimal_idx = np.argmax(tpr - fpr)\n",
    "# #     optimal_threshold = thresholds[optimal_idx]\n",
    "# #     print(\"optimal idx\", optimal_idx, \"optimal threshold\", optimal_threshold)\n",
    "    \n",
    "#     plt.rcParams.update({'figure.figsize': (7, 5)})\n",
    "# #     plt.subplot(1,2,1)\n",
    "# #     plt.plot(m, m_trainerr, 'k', m, m_testerr, 'r')\n",
    "# #     plt.xlabel('Number of Samples', fontsize = 10)\n",
    "# #     plt.ylabel('F1', fontsize = 10)\n",
    "# #     plt.legend(['Training Error', 'Test Error'])\n",
    "#     #plt.subplot(1,2,2)\n",
    "    \n",
    "#     plt.plot(fpr, tpr)\n",
    "#     plt.xlabel('1 - Specificity (FPR)')\n",
    "#     plt.ylabel('Sensitivity (TPR)');\n",
    "#     plt.title(f\"Area Under the ROC Curve: {np.round(roc_auc_score(y2, y2_predictprobs), 4)}\");\n",
    "\n",
    "#     print(f'AUC: {np.round(roc_auc_score(y2, y2_predictprobs), 4)}')\n",
    "\n",
    "\n",
    "#     pyplot.plot(ns2_fpr, ns2_tpr, linestyle='--', label='No Skill')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"xg_aucroc.pdf\")\n",
    "    \n",
    "    \n",
    "#     conf_mat = confusion_matrix(y2, y2_predictprob)\n",
    "#     cm = print_confusion_matrix(conf_mat, ['not a hit', 'is a hit'])\n",
    "    \n",
    "#     return y_pred_prob_df, y_pred_df\n",
    "\n",
    "def plot_model(model, X, y, X2, y2, threshold=0.5):\n",
    "    model.fit(X, y)\n",
    "    y_predict = model.predict(X)\n",
    "    \n",
    "    y_predictprob = (model.predict_proba(X)[:, 1] >= threshold).astype('int')\n",
    "\n",
    "    LR_acc = np.round(np.mean(cross_val_score(model, X, y, scoring='accuracy', cv=5)), 5)\n",
    "    LR_f1 = np.round(np.mean(cross_val_score(model, X, y, scoring='f1', cv=5)), 5)\n",
    "    LR_prec = np.round(np.mean(cross_val_score(model, X, y, scoring='precision', cv=5)), 5)\n",
    "    LR_recall = np.round(np.mean(cross_val_score(model, X, y, scoring='recall', cv=5)), 5)\n",
    "    print(f'The base LR accuracy is: {LR_acc}')\n",
    "    print(f'The base LR f1 is: {LR_f1}')\n",
    "    print(f'The base LR precision is: {LR_prec}')\n",
    "    print(f'The base LR recall is: {LR_recall}')\n",
    "\n",
    "    print(\"Precision: {:6.4f}, Recall: {:6.4f}, Accuracy: {:6.4f}\".format(\n",
    "        precision_score(y, y_predictprob),\n",
    "        recall_score(y, y_predictprob),\n",
    "        accuracy_score(y, y_predictprob)\n",
    "    ))\n",
    "    \n",
    "    m, train_err, test_err = learning_curve(model, X, y, cv=5, scoring='f1', random_state=42)\n",
    "    m_trainerr = np.mean(train_err, axis=1)\n",
    "    m_testerr = np.mean(test_err, axis=1)\n",
    "\n",
    "    print(\"train error\", m_trainerr.mean(), \"test error\", m_testerr.mean())\n",
    "    print(\"y pred proba \", y_predictprob)\n",
    "    \n",
    "    ns2_probs = [0 for _ in range(len(y2))]\n",
    "    y2_predict = model.predict(X2)\n",
    "    y2_predictprobs = model.predict_proba(X2)[:, 1]\n",
    "    y2_predictprob = (model.predict_proba(X2)[:, 1] >= threshold).astype('int')\n",
    "    \n",
    "    print(\"Precision: {:6.4f}, Recall: {:6.4f}, Accuracy: {:6.4f}, f1: {:6.4f}\".format(\n",
    "        precision_score(y2, y2_predictprob),\n",
    "        recall_score(y2, y2_predictprob),\n",
    "        accuracy_score(y2, y2_predictprob),\n",
    "        f1_score(y2, y2_predictprob)\n",
    "    ))\n",
    "    \n",
    "    global y_pred_prob_df\n",
    "    global y_pred_df\n",
    "    y3_predictprob = (model.predict_proba(X2)[:, 1]) \n",
    "    \n",
    "    y_pred_prob_df = pd.DataFrame(y3_predictprob, columns=['y_pred_prob'])\n",
    "    y_pred_df = pd.DataFrame(y2_predictprob, columns=['y_pred'])\n",
    "    \n",
    "    fpr, tpr, thr = roc_curve(y2, y2_predictprobs)\n",
    "    \n",
    "    ns2_auc = roc_auc_score(y2, ns2_probs)\n",
    "    ns2_fpr, ns2_tpr, _ = roc_curve(y2, ns2_probs)\n",
    "\n",
    "    plt.rcParams.update({'figure.figsize': (7, 5)})\n",
    "    \n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('1 - Specificity (FPR)')\n",
    "    plt.ylabel('Sensitivity (TPR)')\n",
    "    plt.title(f\"Area Under the ROC Curve: {np.round(roc_auc_score(y2, y2_predictprobs), 4)}\")\n",
    "\n",
    "    print(f'AUC: {np.round(roc_auc_score(y2, y2_predictprobs), 4)}')\n",
    "\n",
    "    plt.plot(ns2_fpr, ns2_tpr, linestyle='--', label='No Skill')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"xg_aucroc.pdf\")\n",
    "    \n",
    "    conf_mat = confusion_matrix(y2, y2_predictprob)\n",
    "    cm = print_confusion_matrix(conf_mat, ['not a hit', 'is a hit'])\n",
    "    \n",
    "    return y_pred_prob_df, y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base LR accuracy is: 0.76952\n",
      "The base LR f1 is: 0.78239\n",
      "The base LR precision is: 0.73138\n",
      "The base LR recall is: 0.84109\n",
      "Precision: 0.7388, Recall: 0.8502, Accuracy: 0.7781\n",
      "train error 0.8089718602501568 test error 0.7781255461887897\n",
      "y pred proba  [1 0 0 ... 1 0 0]\n",
      "Precision: 0.7463, Recall: 0.8475, Accuracy: 0.7813, f1: 0.7937\n",
      "AUC: 0.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/r2hbd12s1r9ccvfmt_h9r3pm0000gn/T/ipykernel_27611/2678545404.py:38: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      y_pred_prob\n",
       " 0        0.411008\n",
       " 1        0.592546\n",
       " 2        0.550659\n",
       " 3        0.033943\n",
       " 4        0.587934\n",
       " ...           ...\n",
       " 8107     0.705847\n",
       " 8108     0.040766\n",
       " 8109     0.720069\n",
       " 8110     0.020291\n",
       " 8111     0.804120\n",
       " \n",
       " [8112 rows x 1 columns],\n",
       "       y_pred\n",
       " 0          0\n",
       " 1          1\n",
       " 2          1\n",
       " 3          0\n",
       " 4          1\n",
       " ...      ...\n",
       " 8107       1\n",
       " 8108       0\n",
       " 8109       1\n",
       " 8110       0\n",
       " 8111       1\n",
       " \n",
       " [8112 rows x 1 columns])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_base = GradientBoostingClassifier()\n",
    "plot_model(xgb_base, X2_train, y2_train, X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV For Gradient Boosting\n",
    "grad = GradientBoostingClassifier()\n",
    "grad_param = {'n_estimators':[100],\n",
    "              'max_depth':[3,5],\n",
    "              #'max_features': [6,10],\n",
    "              #'min_samples_split': [None], \n",
    "              #'min_samples_leaf':[3,10],\n",
    "              'learning_rate': [0.1] #0.0001, 0.001, 0.01, \n",
    "             }\n",
    "\n",
    "grad_grid = GridSearchCV(grad, param_grid=grad_param, cv=3, scoring='f1', verbose=True, n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "best score:  0.7864014551646356\n",
      "best params:  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "xgb1 = grad_grid.fit(X2_train,y2_train)\n",
    "\n",
    "grad_ypred = grad_grid.predict(X2_test)\n",
    "\n",
    "grad_yproba = grad_grid.predict_proba(X2_test)[:,1]\n",
    "\n",
    "fpr_grad, tpr_grad, _ = roc_curve(y2_test, grad_yproba)\n",
    "\n",
    "print('best score: ',xgb1.best_score_)\n",
    "print('best params: ',xgb1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2=GradientBoostingClassifier(                   \n",
    "                   max_depth=3, \n",
    "                   max_features=4,\n",
    "                   max_leaf_nodes=None,                   \n",
    "                   #min_samples_leaf=10,\n",
    "                   n_estimators=100,\n",
    "                   learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2b = xgb2.fit(X2_train,y2_train)\n",
    "\n",
    "grad_ypred = xgb2.predict(X2_test)\n",
    "\n",
    "grad_yproba = xgb2.predict_proba(X2_test)[:,1]\n",
    "\n",
    "fpr_grad, tpr_grad, _ = roc_curve(y2_test, grad_yproba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base LR accuracy is: 0.76771\n",
      "The base LR f1 is: 0.78034\n",
      "The base LR precision is: 0.73008\n",
      "The base LR recall is: 0.83734\n",
      "Precision: 0.7392, Recall: 0.8457, Accuracy: 0.7770\n",
      "train error 0.8058785524276818 test error 0.7772030583393708\n",
      "y pred proba  [1 0 0 ... 1 0 0]\n",
      "Precision: 0.7432, Recall: 0.8438, Accuracy: 0.7777, f1: 0.7903\n",
      "AUC: 0.8567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/r2hbd12s1r9ccvfmt_h9r3pm0000gn/T/ipykernel_27611/2678545404.py:38: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      y_pred_prob\n",
       " 0        0.396455\n",
       " 1        0.552568\n",
       " 2        0.552963\n",
       " 3        0.039842\n",
       " 4        0.599281\n",
       " ...           ...\n",
       " 8107     0.687593\n",
       " 8108     0.037191\n",
       " 8109     0.745489\n",
       " 8110     0.032372\n",
       " 8111     0.806026\n",
       " \n",
       " [8112 rows x 1 columns],\n",
       "       y_pred\n",
       " 0          0\n",
       " 1          1\n",
       " 2          1\n",
       " 3          0\n",
       " 4          1\n",
       " ...      ...\n",
       " 8107       1\n",
       " 8108       0\n",
       " 8109       1\n",
       " 8110       0\n",
       " 8111       1\n",
       " \n",
       " [8112 rows x 1 columns])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(xgb2, X2_train, y2_train, X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matplotlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m matplotlib\u001b[39m.\u001b[39muse(\u001b[39m'\u001b[39m\u001b[39mTkAgg\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Set the backend to a GUI-compatible backend\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# def plot_variable_importance(model, feature_names, top_n=10):\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#     # Get feature importances from the model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#     feature_importances = model.feature_importances_\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[39m#     plt.show()  # Display the figure\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matplotlib' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "def plot_variable_importance_top10(model, X_train, top_n=10):\n",
    "    from pandas import DataFrame\n",
    "    imp = DataFrame({\"imp\": model.feature_importances_, \"names\": X_train.columns}).sort_values(\"imp\", ascending=True)\n",
    "    imp_top_n = imp[-top_n:]\n",
    "    fig, ax = plt.subplots(figsize=(imp_top_n.shape[0]/6, imp_top_n.shape[0]/5), dpi=300) \n",
    "    ax.barh(imp_top_n[\"names\"], imp_top_n[\"imp\"], color=\"green\") \n",
    "   # ax.set_xlabel('\\nVariable Importance')\n",
    "   # ax.set_ylabel('Features\\n') \n",
    "    ax.set_title('Variable Importance Plot Top 10\\n') \n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "matplotlib.use('TkAgg')  # Set the backend to a GUI-compatible backend\n",
    "\n",
    "\n",
    "# def plot_variable_importance(model, feature_names, top_n=10):\n",
    "#     # Get feature importances from the model\n",
    "#     feature_importances = model.feature_importances_\n",
    "\n",
    "#     # Get the indices of the top N features based on their importance\n",
    "#     top_n_indices = np.argsort(feature_importances)[-top_n:]\n",
    "\n",
    "#     # Get the names of the top N features\n",
    "#     top_n_features = feature_names[top_n_indices]\n",
    "\n",
    "#     # Get the importance values of the top N features\n",
    "#     top_n_importances = feature_importances[top_n_indices]\n",
    "\n",
    "#     # Plotting the variable importance plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.barh(range(len(top_n_features)), top_n_importances, align='center')\n",
    "#     plt.yticks(range(len(top_n_features)), top_n_features)\n",
    "#     plt.xlabel('Importance')\n",
    "#     plt.ylabel('Features')\n",
    "#     plt.title(f'Top {top_n} Variable Importance')\n",
    "\n",
    "#     plt.show()  # Display the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# plot_variable_importance_top10(xgb2, X2_train, top_n=10)\n",
    "\n",
    "plot_variable_importance(xgb2, X2_train, top_n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFITTING BASED ON DROPPED COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X3_train \u001b[39m=\u001b[39m X2_train\u001b[39m.\u001b[39;49mdrop([\u001b[39m'\u001b[39m\u001b[39mtempo\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mspeechiness\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39menergy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msections\u001b[39m\u001b[39m'\u001b[39m], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m#mode, key to be dropped here\u001b[39;00m\n\u001b[1;32m      2\u001b[0m X3_test \u001b[39m=\u001b[39m X2_test\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mtempo\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mspeechiness\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39menergy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msections\u001b[39m\u001b[39m'\u001b[39m], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# rlr_secondf1 = plot_model(lr, lr_Xtrain, y2_train, X2_test, y2_test)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "X3_train = X2_train.drop(['tempo','speechiness','energy', 'sections'], axis = 1) #mode, key to be dropped here\n",
    "X3_test = X2_test.drop(['tempo','speechiness','energy', 'sections'], axis = 1)\n",
    "# rlr_secondf1 = plot_model(lr, lr_Xtrain, y2_train, X2_test, y2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "plot_model(xgb2, X3_train, y2_train, X3_test, y2_test)\n",
    "pickle.dump(xgb2, open('xgb_model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(xgb2, X3_train, y2_train, X3_test, y2_test, 0.66) # changing threshold to 0.66 for more precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF for predicted prob/outcomes and merge back to orig df\n",
    "y_pred_prob_df['row_num']= np.arange(len(y_pred_prob_df))\n",
    "\n",
    "y_pred_df['row_num']= np.arange(len(y_pred_df))\n",
    "X_test['row_num']= np.arange(len(X_test))\n",
    "y_test_df = y_test.to_frame()\n",
    "y_test_df['m_index'] = y_test_df.index\n",
    "#y_test_df2=y_test_df.rename_axis('m_index')\n",
    "y_test_df['row_num']= np.arange(len(y_test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "data_frames=[X_test,y_test_df,y_pred_df,y_pred_prob_df]\n",
    "m_results = reduce(lambda left,right: pd.merge(left,right,on=['row_num'], how='outer'),data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_index'] = df.index\n",
    "final_df = pd.merge(df,m_results,left_on='m_index',right_on='m_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(r'sb_final_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_predict = xgb2.predict(X3_test) #not sure if X3_test is correct\n",
    "hit_score = xgb2.predict_proba(X3_test) #not sure if X3_test is correct\n",
    "print(f'prediction: {hit_predict[0]} \\nprobability of yay: {hit_score[0][1]}\\nprobability of nay: {hit_score[0][0]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
